{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30626,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/issaouimarwa/NewRepo/blob/master/reinforcementlearning_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "\n",
        "import os\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-12-26T05:30:02.669291Z",
          "iopub.execute_input": "2023-12-26T05:30:02.670207Z",
          "iopub.status.idle": "2023-12-26T05:30:03.222568Z",
          "shell.execute_reply.started": "2023-12-26T05:30:02.670168Z",
          "shell.execute_reply": "2023-12-26T05:30:03.221327Z"
        },
        "trusted": true,
        "id": "CtxKnfwErBad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Markov Decision Processes (MDPs)\n",
        "  - A Markov Decision Process is a mathematical model for decision-making in situations where an agent interacts with an environment. It is characterized by the following components:\n",
        "   - State Space (S): The set of all possible situations or configurations.\n",
        "   - Action Space (A): The set of all possible actions the agent can take.\n",
        "   - Transition Probabilities (P): The probability of transitioning from one state to another given a particular action.\n",
        "   - Reward Function (R): The immediate reward the agent receives after taking an action in a particular state.\n",
        "   - Policy (π): A strategy or a mapping from states to actions, defining the agent's behavior.\n",
        "   \n",
        "- In an MDP, the Markov property holds, which means that the future state depends only on the current state and action, not on the sequence of events that preceded them.\n",
        "\n",
        "\n",
        "### Bellman Equations\n",
        " - The Bellman Equations are a set of recursive equations that describe the relationship between the value of a state or state-action pair and the values of its successor states. They play a crucial role in dynamic programming and reinforcement learning.\n",
        "\n",
        "### Bellman Expectation Equation (for Action Values)![image.png](attachment:06efcaf9-bdf6-4e42-99fd-9196fe927edb.png)\n",
        "\n",
        "# Summary\n",
        "\n",
        "- **Reinforcement Learning:** A machine learning paradigm where an agent learns to make decisions by interacting with an environment and receiving feedback in the form of rewards or punishments.\n",
        "\n",
        "- **Markov Decision Processes (MDPs):** A mathematical model for decision-making characterized by state space, action space, transition probabilities, reward function, and policy. It follows the Markov property.\n",
        "\n",
        "- **Bellman Equations:** Recursive equations describing the relationship between the value of a state or state-action pair and the values of its successor states. The Bellman Expectation Equation is crucial in reinforcement learning for both state values and action values.\n"
      ],
      "metadata": {
        "id": "j12gCV98rBae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Programming Policy Evaluation\n",
        "\n",
        "Dynamic Programming (DP) is a technique used in reinforcement learning to solve problems by breaking them down into smaller subproblems and solving them iteratively. Dynamic Programming Policy Evaluation is a specific application of DP in the context of evaluating a policy in a Markov Decision Process (MDP).\n",
        "\n",
        "## Markov Decision Process (MDP)\n",
        "\n",
        "A Markov Decision Process is a mathematical model used to describe decision-making situations where an agent interacts with an environment. It consists of:\n",
        "\n",
        "- **States (S):** Possible situations the agent can be in.\n",
        "- **Actions (A):** Possible moves or decisions the agent can make.\n",
        "- **Transition Probabilities (P):** Probabilities of moving from one state to another after taking an action.\n",
        "- **Rewards (R):** Immediate rewards associated with state-action pairs.\n",
        "- **Policy (π):** A strategy or plan that defines the agent's behavior.\n",
        "\n",
        "## Policy Evaluation\n",
        "\n",
        "Policy Evaluation is the process of determining the expected return of a given policy in an MDP. Dynamic Programming Policy Evaluation focuses on iteratively estimating the state values for a given policy. The state value is the expected return when starting in a particular state and following the given policy.\n",
        "\n",
        "### Algorithm\n",
        "\n",
        "The iterative process for Dynamic Programming Policy Evaluation can be described as follows:\n",
        "\n",
        "1. **Initialize:** Set the initial estimates for all state values arbitrarily.\n",
        "2. **Iterate:** Update the value of each state using the Bellman Expectation Equation.\n",
        "   - Bellman Expectation Equation: \\( V(s) = \\sum_{a} \\pi(a|s) \\sum_{s', r} P(s', r|s, a) [r + \\gamma V(s')] \\)\n",
        "   - \\( V(s) \\): Value of state \\( s \\)\n",
        "   - \\( \\pi(a|s) \\): Probability of taking action \\( a \\) in state \\( s \\) according to the policy \\( \\pi \\)\n",
        "   - \\( P(s', r|s, a) \\): Transition probability to reach state \\( s' \\) with reward \\( r \\) after taking action \\( a \\) in state \\( s \\)\n",
        "   - \\( \\gamma \\): Discount factor for future rewards.\n",
        "3. **Repeat:** Iterate until the state values converge or reach a predefined threshold.\n",
        "\n",
        "### Example\n",
        "\n",
        "Consider a grid-world environment where an agent can move left, right, up, or down. The goal is to find the optimal policy that maximizes the expected return. The DP Policy Evaluation algorithm can be applied to estimate the state values under a given policy, helping the agent make informed decisions.\n",
        "\n",
        "Dynamic Programming Policy Evaluation is a fundamental step in reinforcement learning, laying the groundwork for more advanced algorithms such as Policy Iteration and Value Iteration.\n",
        "\n"
      ],
      "metadata": {
        "id": "6J0NZbvErBag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install envs"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T05:55:19.979071Z",
          "iopub.execute_input": "2023-12-26T05:55:19.979527Z",
          "iopub.status.idle": "2023-12-26T05:55:36.196668Z",
          "shell.execute_reply.started": "2023-12-26T05:55:19.979493Z",
          "shell.execute_reply": "2023-12-26T05:55:36.195139Z"
        },
        "_kg_hide-output": true,
        "trusted": true,
        "id": "hw2oTp6ZrBah",
        "outputId": "d1f8e0b9-7f4a-469a-85b6-b49d49cc92a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting envs\n  Downloading envs-1.4-py3-none-any.whl (10 kB)\nInstalling collected packages: envs\nSuccessfully installed envs-1.4\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Evaluation in Gridworld\n",
        "\n",
        "This code implements policy evaluation for a simple gridworld environment using dynamic programming. The gridworld is a classic example where an agent navigates a grid, receiving rewards and updating its state values based on a given policy.\n",
        "\n",
        "## Gridworld Environment\n",
        "\n",
        "The `GridworldEnv` class represents the gridworld environment. It has the following attributes and methods:\n",
        "\n",
        "- `shape`: Tuple representing the gridworld dimensions.\n",
        "- `nS`: Number of states in the gridworld.\n",
        "- `nA`: Number of actions (left, right, up, down).\n",
        "- `P`: Transition probabilities, a dictionary representing the dynamics of the environment.\n",
        "\n",
        "The `_build_transition_matrix` method initializes the transition probabilities for each state-action pair.\n",
        "\n",
        "## Policy Evaluation Function\n",
        "\n",
        "The `policy_evaluation` function takes a policy, the gridworld environment, and optional parameters:\n",
        "\n",
        "- `policy`: 2D array representing the policy (actions for each state).\n",
        "- `env`: Gridworld environment.\n",
        "- `theta`: Convergence threshold.\n",
        "- `discount_factor`: Discount factor for future rewards.\n",
        "\n",
        "The function iteratively updates the state values using the Bellman Expectation Equation until convergence.\n",
        "\n",
        "## Example Usage\n",
        "\n",
        "In the example usage section:\n",
        "\n",
        "1. An instance of the `GridworldEnv` class is created, representing a 4x4 gridworld.\n",
        "2. A random policy is defined (uniform distribution over actions).\n",
        "3. Policy evaluation is performed using the `policy_evaluation` function.\n",
        "4. The resulting state values are printed, reshaped to match the gridworld shape.\n",
        "\n",
        "- code serves as a basic template for policy evaluation in a gridworld environment. It can be extended or modified based on specific gridworld scenarios or other reinforcement learning environments.\n"
      ],
      "metadata": {
        "id": "qXMKLxgHrBai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy Evaluation in Python (Gridworld)\n",
        "\n",
        "class GridworldEnv:\n",
        "    def __init__(self):\n",
        "        self.shape = (4, 4)\n",
        "        self.nS = np.prod(self.shape)  # Number of states\n",
        "        self.nA = 4  # Number of actions (left, right, up, down)\n",
        "        self.P = self._build_transition_matrix()  # Transition probabilities\n",
        "\n",
        "    def _build_transition_matrix(self):\n",
        "        P = {}\n",
        "\n",
        "        for s in range(self.nS):\n",
        "            P[s] = {a: [] for a in range(self.nA)}\n",
        "\n",
        "        def add_transition(s, a, s_, prob, reward, done):\n",
        "            P[s][a].append((prob, s_, reward, done))\n",
        "\n",
        "        for i in range(self.shape[0]):\n",
        "            for j in range(self.shape[1]):\n",
        "                s = i * self.shape[1] + j\n",
        "\n",
        "                # Define possible actions (left, right, up, down)\n",
        "                for a in range(self.nA):\n",
        "                    if a == 0:  # Left\n",
        "                        s_ = max(j - 1, 0)\n",
        "                    elif a == 1:  # Right\n",
        "                        s_ = min(j + 1, self.shape[1] - 1)\n",
        "                    elif a == 2:  # Up\n",
        "                        s_ = max(i - 1, 0)\n",
        "                    elif a == 3:  # Down\n",
        "                        s_ = min(i + 1, self.shape[0] - 1)\n",
        "\n",
        "                    # Probabilities, next state, reward, done\n",
        "                    prob = 1.0 if s_ == s else 0.0\n",
        "                    reward = 0.0 if s_ != s else -1.0\n",
        "                    done = False\n",
        "\n",
        "                    add_transition(s, a, s_, prob, reward, done)\n",
        "\n",
        "        return P\n",
        "\n",
        "def policy_evaluation(policy, env, theta=1e-6, discount_factor=0.9):\n",
        "    num_states = env.nS\n",
        "\n",
        "    # Initialize state values arbitrarily\n",
        "    V = np.zeros(num_states)\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(num_states):\n",
        "            v = 0\n",
        "            # Accumulate value for each action in the policy\n",
        "            for a, action_prob in enumerate(policy[s]):\n",
        "                for prob, next_state, reward, done in env.P[s][a]:\n",
        "                    v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
        "\n",
        "            delta = max(delta, np.abs(v - V[s]))\n",
        "            V[s] = v\n",
        "\n",
        "        # Check for convergence\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return V\n",
        "\n",
        "# use:\n",
        "if __name__ == \"__main__\":\n",
        "    env = GridworldEnv()\n",
        "\n",
        "    # Define a random policy (uniform distribution over actions)\n",
        "    random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
        "\n",
        "    # Perform policy evaluation\n",
        "    state_values = policy_evaluation(random_policy, env)\n",
        "\n",
        "    # Print the resulting state values\n",
        "    print(\"State Values:\")\n",
        "    print(state_values.reshape(env.shape))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T06:04:20.144342Z",
          "iopub.execute_input": "2023-12-26T06:04:20.144852Z",
          "iopub.status.idle": "2023-12-26T06:04:20.175551Z",
          "shell.execute_reply.started": "2023-12-26T06:04:20.144815Z",
          "shell.execute_reply": "2023-12-26T06:04:20.174149Z"
        },
        "trusted": true,
        "id": "g6upHbVWrBai",
        "outputId": "70887f52-a11f-4c82-ce90-2bb3597117f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "State Values:\n[[-0.90909039 -0.32258065  0.         -0.32258065]\n [ 0.          0.          0.          0.        ]\n [ 0.          0.          0.          0.        ]\n [ 0.          0.          0.          0.        ]]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Iteration in Gridworld\n",
        "\n",
        "Policy Iteration is an iterative algorithm used in reinforcement learning to find the optimal policy for a given Markov Decision Process (MDP). It alternates between two steps: policy evaluation and policy improvement.\n",
        "\n",
        "## Gridworld Environment\n",
        "\n",
        "The `GridworldEnv` class represents the gridworld environment. It includes the grid shape, the number of states (`nS`), the number of actions (`nA`), the transition probabilities (`P`), and the current policy.\n",
        "\n",
        "## Policy Evaluation\n",
        "\n",
        "The `policy_evaluation` function iteratively estimates the state values under the current policy. It applies the Bellman Expectation Equation until the values converge within a specified threshold (`theta`).\n",
        "\n",
        "## Policy Improvement\n",
        "\n",
        "The `policy_improvement` function takes the current policy, state values, and discount factor as inputs. It greedily selects the action that maximizes the expected return for each state, resulting in an improved policy.\n",
        "\n",
        "## Policy Iteration Algorithm\n",
        "\n",
        "The `policy_iteration` function combines policy evaluation and improvement. It iteratively performs policy evaluation and improvement until the policy converges. The algorithm stops when the policy no longer changes.\n",
        "\n",
        "### Example Usage\n",
        "\n",
        "In the example usage section:\n",
        "\n",
        "1. An instance of the `GridworldEnv` class is created, representing a 4x4 gridworld.\n",
        "2. Policy Iteration is performed using the `policy_iteration` function.\n",
        "3. The resulting optimal policy and state values are printed.\n",
        "\n",
        "This code provides a basic template for understanding and implementing Policy Iteration in a gridworld environment. Adjustments can be made based on specific gridworld scenarios or other reinforcement learning environments.\n"
      ],
      "metadata": {
        "id": "kCg1n_lerBaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy Iteration in Python (Gridworld)\n",
        "import numpy as np\n",
        "\n",
        "class GridworldEnv:\n",
        "    def __init__(self):\n",
        "        self.shape = (4, 4)\n",
        "        self.nS = np.prod(self.shape)  # Number of states\n",
        "        self.nA = 4  # Number of actions (left, right, up, down)\n",
        "        self.P = self._build_transition_matrix()  # Transition probabilities\n",
        "        self.policy = np.ones([self.nS, self.nA]) / self.nA  # Initialize a random policy\n",
        "\n",
        "    def _build_transition_matrix(self):\n",
        "        P = {}\n",
        "\n",
        "        for s in range(self.nS):\n",
        "            P[s] = {a: [] for a in range(self.nA)}\n",
        "\n",
        "        def add_transition(s, a, s_, prob, reward, done):\n",
        "            P[s][a].append((prob, s_, reward, done))\n",
        "\n",
        "        for i in range(self.shape[0]):\n",
        "            for j in range(self.shape[1]):\n",
        "                s = i * self.shape[1] + j\n",
        "\n",
        "                # Define possible actions (left, right, up, down)\n",
        "                for a in range(self.nA):\n",
        "                    if a == 0:  # Left\n",
        "                        s_ = max(j - 1, 0)\n",
        "                    elif a == 1:  # Right\n",
        "                        s_ = min(j + 1, self.shape[1] - 1)\n",
        "                    elif a == 2:  # Up\n",
        "                        s_ = max(i - 1, 0)\n",
        "                    elif a == 3:  # Down\n",
        "                        s_ = min(i + 1, self.shape[0] - 1)\n",
        "\n",
        "                    # Probabilities, next state, reward, done\n",
        "                    prob = 1.0 if s_ == s else 0.0\n",
        "                    reward = 0.0 if s_ != s else -1.0\n",
        "                    done = False\n",
        "\n",
        "                    add_transition(s, a, s_, prob, reward, done)\n",
        "\n",
        "        return P\n",
        "\n",
        "def policy_evaluation(policy, env, theta=1e-6, discount_factor=0.9):\n",
        "    num_states = env.nS\n",
        "\n",
        "    # Initialize state values arbitrarily\n",
        "    V = np.zeros(num_states)\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(num_states):\n",
        "            v = 0\n",
        "            # Accumulate value for each action in the policy\n",
        "            for a, action_prob in enumerate(policy[s]):\n",
        "                for prob, next_state, reward, done in env.P[s][a]:\n",
        "                    v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
        "\n",
        "            delta = max(delta, np.abs(v - V[s]))\n",
        "            V[s] = v\n",
        "\n",
        "        # Check for convergence\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return V\n",
        "\n",
        "def policy_improvement(policy, env, V, discount_factor=0.9):\n",
        "    num_states = env.nS\n",
        "    num_actions = env.nA\n",
        "\n",
        "    new_policy = np.zeros([num_states, num_actions]) / num_actions\n",
        "\n",
        "    for s in range(num_states):\n",
        "        # Find the best action (argmax) based on the current value function\n",
        "        best_action = np.argmax([sum(prob * (reward + discount_factor * V[next_state]) for prob, next_state, reward, _ in env.P[s][a]) for a in range(num_actions)])\n",
        "        new_policy[s][best_action] = 1.0\n",
        "\n",
        "    return new_policy\n",
        "\n",
        "def policy_iteration(env, theta=1e-6, discount_factor=0.9, max_iterations=1000):\n",
        "    for i in range(max_iterations):\n",
        "        # Policy Evaluation\n",
        "        V = policy_evaluation(env.policy, env, theta, discount_factor)\n",
        "\n",
        "        # Policy Improvement\n",
        "        new_policy = policy_improvement(env.policy, env, V, discount_factor)\n",
        "\n",
        "        # Check if the policy has converged\n",
        "        if np.array_equal(new_policy, env.policy):\n",
        "            break\n",
        "\n",
        "        # Update the policy\n",
        "        env.policy = new_policy\n",
        "\n",
        "    return env.policy, V\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    env = GridworldEnv()\n",
        "\n",
        "    # Perform Policy Iteration\n",
        "    optimal_policy, optimal_values = policy_iteration(env)\n",
        "\n",
        "    # Print the resulting optimal policy and state values\n",
        "    print(\"Optimal Policy:\")\n",
        "    print(optimal_policy)\n",
        "    print(\"\\nOptimal State Values:\")\n",
        "    print(optimal_values.reshape(env.shape))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T06:11:08.398069Z",
          "iopub.execute_input": "2023-12-26T06:11:08.398560Z",
          "iopub.status.idle": "2023-12-26T06:11:08.431542Z",
          "shell.execute_reply.started": "2023-12-26T06:11:08.398525Z",
          "shell.execute_reply": "2023-12-26T06:11:08.430327Z"
        },
        "trusted": true,
        "id": "WZyeGhAprBaj",
        "outputId": "184b46f0-6bca-4393-b6a2-f2b56631a34a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Optimal Policy:\n[[0. 1. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]]\n\nOptimal State Values:\n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Value Iteration is another iterative algorithm used in reinforcement learning to find the optimal policy for a given Markov Decision Process (MDP). It involves iteratively updating the state values until convergence.\n",
        "\n",
        "\n",
        "# Value Iteration in Gridworld\n",
        "\n",
        "Value Iteration is an iterative algorithm used in reinforcement learning to find the optimal policy for a Markov Decision Process (MDP). It alternates between updating the state values and extracting the optimal policy until convergence.\n",
        "\n",
        "## Gridworld Environment\n",
        "\n",
        "The `GridworldEnv` class represents the gridworld environment. It includes the grid shape, the number of states (`nS`), the number of actions (`nA`), and the transition probabilities (`P`).\n",
        "\n",
        "## Value Iteration Function\n",
        "\n",
        "The `value_iteration` function takes the gridworld environment, convergence threshold (`theta`), discount factor (`discount_factor`), and the maximum number of iterations (`max_iterations`) as inputs. It iteratively updates the state values until convergence and extracts the optimal policy.\n",
        "\n",
        "- `num_states`: Number of states in the gridworld.\n",
        "- `num_actions`: Number of actions (left, right, up, down).\n",
        "- `V`: Array representing the state values.\n",
        "- `delta`: Convergence measure.\n",
        "\n",
        "The algorithm stops when the change in state values is smaller than the specified threshold.\n",
        "\n",
        "### Example Usage\n",
        "\n",
        "In the example usage section:\n",
        "\n",
        "1. An instance of the `GridworldEnv` class is created, representing a 4x4 gridworld.\n",
        "2. Value Iteration is performed using the `value_iteration` function.\n",
        "3. The resulting optimal policy and state values are printed.\n",
        "\n",
        "This code serves as a basic template for understanding and implementing Value Iteration in a gridworld environment. Adjustments can be made based on specific gridworld scenarios or other reinforcement learning environments.\n"
      ],
      "metadata": {
        "id": "7ICjh0ikrBak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement Value Iteration in Python (Gridworld)\n",
        "class GridworldEnv:\n",
        "    def __init__(self):\n",
        "        self.shape = (4, 4)\n",
        "        self.nS = np.prod(self.shape)  # Number of states\n",
        "        self.nA = 4  # Number of actions (left, right, up, down)\n",
        "        self.P = self._build_transition_matrix()  # Transition probabilities\n",
        "\n",
        "    def _build_transition_matrix(self):\n",
        "        P = {}\n",
        "\n",
        "        for s in range(self.nS):\n",
        "            P[s] = {a: [] for a in range(self.nA)}\n",
        "\n",
        "        def add_transition(s, a, s_, prob, reward, done):\n",
        "            P[s][a].append((prob, s_, reward, done))\n",
        "\n",
        "        for i in range(self.shape[0]):\n",
        "            for j in range(self.shape[1]):\n",
        "                s = i * self.shape[1] + j\n",
        "\n",
        "                # Define possible actions (left, right, up, down)\n",
        "                for a in range(self.nA):\n",
        "                    if a == 0:  # Left\n",
        "                        s_ = max(j - 1, 0)\n",
        "                    elif a == 1:  # Right\n",
        "                        s_ = min(j + 1, self.shape[1] - 1)\n",
        "                    elif a == 2:  # Up\n",
        "                        s_ = max(i - 1, 0)\n",
        "                    elif a == 3:  # Down\n",
        "                        s_ = min(i + 1, self.shape[0] - 1)\n",
        "\n",
        "                    # Probabilities, next state, reward, done\n",
        "                    prob = 1.0 if s_ == s else 0.0\n",
        "                    reward = 0.0 if s_ != s else -1.0\n",
        "                    done = False\n",
        "\n",
        "                    add_transition(s, a, s_, prob, reward, done)\n",
        "\n",
        "        return P\n",
        "\n",
        "def value_iteration(env, theta=1e-6, discount_factor=0.9, max_iterations=1000):\n",
        "    num_states = env.nS\n",
        "    num_actions = env.nA\n",
        "\n",
        "    # Initialize state values arbitrarily\n",
        "    V = np.zeros(num_states)\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        delta = 0\n",
        "        for s in range(num_states):\n",
        "            # Compute the new value for each state using the Bellman Optimality Equation\n",
        "            v = max([sum(prob * (reward + discount_factor * V[next_state]) for prob, next_state, reward, _ in env.P[s][a]) for a in range(num_actions)])\n",
        "\n",
        "            delta = max(delta, np.abs(v - V[s]))\n",
        "            V[s] = v\n",
        "\n",
        "        # Check for convergence\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    # Extract and return the optimal policy from the computed state values\n",
        "    optimal_policy = np.zeros([num_states, num_actions])\n",
        "    for s in range(num_states):\n",
        "        best_action = np.argmax([sum(prob * (reward + discount_factor * V[next_state]) for prob, next_state, reward, _ in env.P[s][a]) for a in range(num_actions)])\n",
        "        optimal_policy[s][best_action] = 1.0\n",
        "\n",
        "    return optimal_policy, V\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    env = GridworldEnv()\n",
        "\n",
        "    # Perform Value Iteration\n",
        "    optimal_policy, optimal_values = value_iteration(env)\n",
        "\n",
        "    # Print the resulting optimal policy and state values\n",
        "    print(\"Optimal Policy:\")\n",
        "    print(optimal_policy)\n",
        "    print(\"\\nOptimal State Values:\")\n",
        "    print(optimal_values.reshape(env.shape))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T06:21:30.083707Z",
          "iopub.execute_input": "2023-12-26T06:21:30.084224Z",
          "iopub.status.idle": "2023-12-26T06:21:30.110533Z",
          "shell.execute_reply.started": "2023-12-26T06:21:30.084176Z",
          "shell.execute_reply": "2023-12-26T06:21:30.109499Z"
        },
        "trusted": true,
        "id": "3OLZft2lrBak",
        "outputId": "1154bbdb-f320-460b-e011-e72a81bdb738"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Optimal Policy:\n[[0. 1. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]]\n\nOptimal State Values:\n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Gambler's Problem is a classic reinforcement learning problem where a gambler has the opportunity to make bets on a coin flip, aiming to reach a target amount of money. The goal is to find the optimal policy for making bets to maximize the chances of reaching the target.\n",
        "\n",
        "\n",
        "\n",
        "# Gambler's Problem using Value Iteration\n",
        "\n",
        "The Gambler's Problem is a classic reinforcement learning scenario where a gambler aims to reach a target amount of money by making bets on a coin flip. The goal is to find the optimal policy for making bets that maximizes the chances of reaching the target.\n",
        "\n",
        "## Value Iteration Function\n",
        "\n",
        "The `value_iteration_gamblers_problem` function performs the Value Iteration algorithm for the Gambler's Problem. It takes the target amount, the probability of heads (`p_heads`), convergence threshold (`theta`), and discount factor (`discount_factor`) as inputs. The function iteratively updates the state values and extracts the optimal policy.\n",
        "\n",
        "- `calculate_reward`: Function to calculate the reward for a state-action pair.\n",
        "- `calculate_next_state`: Function to calculate the next state based on the current state and action.\n",
        "- The main loop updates state values until convergence.\n",
        "\n",
        "## Plotting Function\n",
        "\n",
        "The `plot_gamblers_problem_solution` function is used to visualize the results. It plots the value function and the optimal policy against the gambler's capital.\n",
        "\n",
        "## Example Usage\n",
        "\n",
        "In the example usage section:\n",
        "\n",
        "1. Set the target amount (`target_amount`) and call `value_iteration_gamblers_problem` to obtain the optimal policy and value function.\n",
        "2. Print the optimal policy and plot the results using `plot_gamblers_problem_solution`.\n",
        "\n",
        "This code provides a basic implementation of the Gambler's Problem using the Value Iteration algorithm. It is a useful template for understanding and experimenting with reinforcement learning in this specific scenario. Feel free to modify parameters or explore additional features based on your needs.\n"
      ],
      "metadata": {
        "id": "opk1lZHerBak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def value_iteration_gamblers_problem(target, p_heads=0.4, theta=1e-9, discount_factor=1.0):\n",
        "#     max_states = target + 1\n",
        "#     V = np.zeros(max_states)\n",
        "#     policy = np.zeros(max_states)\n",
        "\n",
        "#     def calculate_reward(state, action):\n",
        "#         # Reward is 1 if the target is reached, else 0\n",
        "#         return 1.0 if state + action == target else 0.0\n",
        "\n",
        "#     def calculate_next_state(state, action):\n",
        "#         # Next state after the coin flip\n",
        "#         return state + action if np.random.rand() < p_heads else state - action\n",
        "\n",
        "#     while True:\n",
        "#         delta = 0\n",
        "#         for s in range(1, target):  # Exclude states 0 and target (terminal states)\n",
        "#             v = V[s]\n",
        "#             max_value = 0\n",
        "#             for a in range(1, min(s, target - s) + 1):  # Possible actions\n",
        "#                 next_state_value = p_heads * (calculate_reward(s, a) + discount_factor * V[calculate_next_state(s, a)]) + \\\n",
        "#                                    (1 - p_heads) * (calculate_reward(s, -a) + discount_factor * V[calculate_next_state(s, -a)])\n",
        "#                 if next_state_value > max_value:\n",
        "#                     max_value = next_state_value\n",
        "\n",
        "#             V[s] = max_value\n",
        "#             delta = max(delta, np.abs(v - V[s]))\n",
        "\n",
        "#         if delta < theta:\n",
        "#             break\n",
        "\n",
        "#     # Extract the optimal policy\n",
        "#     for s in range(1, target):\n",
        "#         max_action = 0\n",
        "#         max_value = 0\n",
        "#         for a in range(1, min(s, target - s) + 1):\n",
        "#             next_state_value = p_heads * (calculate_reward(s, a) + discount_factor * V[calculate_next_state(s, a)]) + \\\n",
        "#                                (1 - p_heads) * (calculate_reward(s, -a) + discount_factor * V[calculate_next_state(s, -a)])\n",
        "#             if next_state_value > max_value:\n",
        "#                 max_value = next_state_value\n",
        "#                 max_action = a\n",
        "\n",
        "#         policy[s] = max_action\n",
        "\n",
        "#     return policy, V\n",
        "\n",
        "# def plot_gamblers_problem_solution(policy, value_function):\n",
        "#     plt.figure(figsize=(10, 5))\n",
        "\n",
        "#     plt.subplot(2, 1, 1)\n",
        "#     plt.plot(range(1, len(value_function) - 1), value_function[1:-1], marker='o')\n",
        "#     plt.title(\"Value Function\")\n",
        "#     plt.xlabel(\"Capital\")\n",
        "#     plt.ylabel(\"Value\")\n",
        "\n",
        "#     plt.subplot(2, 1, 2)\n",
        "#     plt.scatter(range(1, len(policy) - 1), policy[1:-1], marker='o')\n",
        "#     plt.title(\"Optimal Policy\")\n",
        "#     plt.xlabel(\"Capital\")\n",
        "#     plt.ylabel(\"Stake\")\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# # Example usage:\n",
        "# if __name__ == \"__main__\":\n",
        "#     target_amount = 100\n",
        "#     optimal_policy, optimal_value_function = value_iteration_gamblers_problem(target_amount)\n",
        "\n",
        "#     # Print and plot the results\n",
        "#     print(\"Optimal Policy:\")\n",
        "#     print(optimal_policy[1:])  # Exclude states 0 and target\n",
        "#     plot_gamblers_problem_solution(optimal_policy, optimal_value_function)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T06:28:27.291162Z",
          "iopub.execute_input": "2023-12-26T06:28:27.291831Z",
          "iopub.status.idle": "2023-12-26T06:28:27.301368Z",
          "shell.execute_reply.started": "2023-12-26T06:28:27.291787Z",
          "shell.execute_reply": "2023-12-26T06:28:27.299930Z"
        },
        "trusted": true,
        "id": "OqAJ6NNKrBal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Value Iteration can be computationally expensive, especially for larger state spaces. If the code is taking longer than expected to run, you might consider a few optimizations:\n",
        "\n",
        "    - Parallelization: If your machine has multiple cores, you can consider parallelizing the loop that iterates over states. The joblib library is a popular choice for parallelizing loops in Python.\n",
        "\n",
        "    - Numpy Vectorization: Numpy operations are highly optimized and can be faster than using explicit loops. Try to use vectorized operations wherever possible.\n",
        "\n",
        "    - Memory Efficiency: Ensure that you are not using excessive memory. If memory becomes a bottleneck, consider more memory-efficient data structures.\n",
        "\n",
        "    - Convergence Threshold: You might try adjusting the convergence threshold (theta). A smaller threshold might lead to more precise results but may require more iterations."
      ],
      "metadata": {
        "id": "l_AJQ2DmrBal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# value_iteration_gamblers_problem function using Numpy vectorization and joblib parallelization\n",
        "import numpy as np\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "def value_iteration_gamblers_problem_parallel(target, p_heads=0.4, theta=1e-9, discount_factor=1.0, num_cores=1):\n",
        "    max_states = target + 1\n",
        "    V = np.zeros(max_states)\n",
        "    policy = np.zeros(max_states)\n",
        "\n",
        "    def calculate_reward(state, action):\n",
        "        return 1.0 if state + action == target else 0.0\n",
        "\n",
        "    def calculate_next_state(state, action):\n",
        "        return state + action if np.random.rand() < p_heads else state - action\n",
        "\n",
        "    def update_state(s):\n",
        "        max_value = 0\n",
        "        for a in range(1, min(s, target - s) + 1):\n",
        "            next_state_value = p_heads * (calculate_reward(s, a) + discount_factor * V[calculate_next_state(s, a)]) + \\\n",
        "                               (1 - p_heads) * (calculate_reward(s, -a) + discount_factor * V[calculate_next_state(s, -a)])\n",
        "            max_value = max(max_value, next_state_value)\n",
        "\n",
        "        return max_value\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "\n",
        "        # Parallelize the state update loop\n",
        "        updated_values = Parallel(n_jobs=num_cores)(delayed(update_state)(s) for s in range(1, target))\n",
        "\n",
        "        for s, updated_value in zip(range(1, target), updated_values):\n",
        "            v = V[s]\n",
        "            V[s] = updated_value\n",
        "            delta = max(delta, np.abs(v - V[s]))\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    # Extract the optimal policy\n",
        "    for s in range(1, target):\n",
        "        max_action = np.argmax([p_heads * (calculate_reward(s, a) + discount_factor * V[calculate_next_state(s, a)]) +\n",
        "                                (1 - p_heads) * (calculate_reward(s, -a) + discount_factor * V[calculate_next_state(s, -a)])\n",
        "                                for a in range(1, min(s, target - s) + 1)])\n",
        "        policy[s] = max_action\n",
        "\n",
        "    return policy, V\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    target_amount = 100\n",
        "    optimal_policy, optimal_value_function = value_iteration_gamblers_problem_parallel(target_amount, num_cores=2)\n",
        "\n",
        "    # Print and plot the results\n",
        "    print(\"Optimal Policy:\")\n",
        "    print(optimal_policy[1:])  # Exclude states 0 and target\n",
        "    plot_gamblers_problem_solution(optimal_policy, optimal_value_function)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-26T06:30:46.362821Z",
          "iopub.execute_input": "2023-12-26T06:30:46.363323Z",
          "iopub.status.idle": "2023-12-26T06:30:46.372369Z",
          "shell.execute_reply.started": "2023-12-26T06:30:46.363286Z",
          "shell.execute_reply": "2023-12-26T06:30:46.371033Z"
        },
        "trusted": true,
        "id": "_fao22ourBal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3f0v0z9UrBal"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}